{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f0fa0-6584-44d4-b163-a891252769f9",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 3 - Tree-Based Models\n",
    "\n",
    "Hello everyone, this assignment notebook covers Tree-Based Models. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Rename and save this Jupyter notebook as **cs5228_a3_YourName_YourNUSNETID.ipynb** (e.g., **cs5228_a3_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Rename and save the script file *cs5228_a3_script.py* as **cs5228_a3_YourName_YourNUSNETID.py** (e.g., **cs5228_a3_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Oct 26, 11.59 pm. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use you full name if its a rather long; it's just  important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a3b10-7afe-4814-9a79-32310842f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0286970M'\n",
    "nusnet_id = 'E1268038'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7455d1c-31ad-4620-8d8a-ad2adc363f56",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Bagging and Feature Sampling (10 Points)**\n",
    "    * 1.1 Implement Bootstrapping (2 Points)\n",
    "    * 1.2 Implement Feature Sampling (2 Points)\n",
    "    * 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "* **2 Implementing AdaBoost with Decision Stumps (30 Points)**\n",
    "    * 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "        * 2.1 a) Calculating the Gini Score a Single Node (2 Points)\n",
    "        * 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "        * 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "        * 2.1 d) Predicting the Classes (3 Points)\n",
    "    * 2.2 Implementing AdaBoost (12 Points)\n",
    "        * 2.2 a) Training the Gradient-Boosted Regressor (8 Points)\n",
    "        * 2.2 b) Predicting Output Values (4 Points)\n",
    "    * 2.3 Questions about AdaBoost (6 Points)\n",
    "        * 2.3 a) Question 1 (2 Points)\n",
    "        * 2.3 b) Question 2 (4 Points)\n",
    "* **3 Evaluation of Tree-Based Models (10 Points)**\n",
    "    * 3.1 Data Preprocessing (2 Points)\n",
    "    * 3.2 Basic K-Fold Cross Validation (8 Points)\n",
    "        * 3.2a) Comparing Tree-Based Regression Models (5 Points)\n",
    "        * 3.2b) Assessing the Evaluation (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4e6d6-30f8-4722-9750-4986c33fcccd",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Enable Auto-Reload\n",
    "\n",
    "This ensures that any saved changes to your `.py` file gets automatically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65641534-c0db-491c-8f9a-475ba94a5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad660cb-6a44-4906-8756-d18ec238bf09",
   "metadata": {},
   "source": [
    "### Enable \"Inline Plotting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8067a9c2-d8d6-4cb5-b994-07fdc2c51b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd634-a20d-491b-9339-49eab3ab7205",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca2bb72-8aa3-4d53-9d2a-e4953aa02592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befecf0d-9ea3-4355-abb3-6cc7b1daa7ec",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `cs5228_a3.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0827e5-fcc6-42d4-93df-96258a07cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs5228_a3_WuXiangjiekang_e1268038 import *\n",
    "#from cs5228_a3_BobSmith_e12345678 import get_noise_dbscan # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ccaa8-0df8-4b2c-9676-0f872ec645d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65ca2e-36ac-4d5e-aa5d-b5c5e39ba1a4",
   "metadata": {},
   "source": [
    "## 1 Bagging and Feature Sampling (10 Points)\n",
    "\n",
    "In the lecture, we discussed the limitations of individual Decision Trees, which motivated the notion of Tree Ensembles. In a nutshell, a Tree Ensemble trains multiple Decision Trees within the same classifier and regressor to reduce variance and improve accuracy. The first approach towards creating Tree Ensembles was to train multiple Decision Trees over different samples of the data:\n",
    "\n",
    "* **Bagging (Bootstrap Aggregation):** Sample a new dataset $D_i$ sampled from $D$ uniformly and with replacement ($|D_i| = |D|$)\n",
    "* **Feature Sampling:** For a given dataset $D$ with $d$ features, consider only a random subset of features of size $m$ with $m<d$.\n",
    "\n",
    "Combining Bagging and Feature Sampling is the underlying idea of *Random Forests*. In this task, you will explore the effects of Bagging and Bagging+FeatureSampling\n",
    "\n",
    "We use the very basic [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset: it's small and clean, and has only numerical features. The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
    "\n",
    "### Prepare Example Data\n",
    "\n",
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a94584c-9786-461d-8a35-290468e44fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0\n",
       "4           5.0          3.6           1.4          0.2        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-iris.csv')\n",
    "\n",
    "# Convert the species name to numerical categories 0, 1, 2\n",
    "df['species'] = pd.factorize(df['species'])[0]\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5be1d5-ec05-4775-b65b-49072c231f5e",
   "metadata": {},
   "source": [
    "#### Convert to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cfc1fb-102b-4daf-be5e-39c5d8196d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n"
     ]
    }
   ],
   "source": [
    "data = df.to_numpy()\n",
    "\n",
    "X = data[:,0:4]\n",
    "y = data[:,4].astype(int)\n",
    "\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e33f-34d9-4925-9955-66fb240ce08e",
   "metadata": {},
   "source": [
    "### 1.1 Implement Bootstrapping (2 Points)\n",
    "\n",
    "Implement method `create_boostrap_sample()` to generate a bootstrap sample for a given dataset! The input dataset is represented by feature array `X` and array `y` containing the class labels (classification) or output values (regression). Hint: numpy provides some convenient methods to make this a very simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eaec19-c816-44a7-af9c-3f50d8661b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_bootstrap: (150, 4)\n",
      "Shape of y_bootstrap: (150,)\n"
     ]
    }
   ],
   "source": [
    "def create_bootstrap_sample(X, y):\n",
    "    X_bootstrap, y_bootstrap = None, None\n",
    "\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_bootstrap, y_bootstrap = None, None\n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################\n",
    "    # Generate random indices with replacement to create the bootstrap sample\n",
    "    bootstrap_indices = np.random.choice(N, size=N, replace=True)\n",
    "    \n",
    "    # Use the random indices to select data points from X and y\n",
    "    X_bootstrap = X[bootstrap_indices]\n",
    "    y_bootstrap = y[bootstrap_indices]\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################\n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "\n",
    "X_bootstrap, y_bootstrap = create_bootstrap_sample(X, y)\n",
    "\n",
    "print('Shape of X_bootstrap: {}'.format(X_bootstrap.shape))\n",
    "print('Shape of y_bootstrap: {}'.format(y_bootstrap.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e2b42-0489-4e61-93da-ba054196ab83",
   "metadata": {},
   "source": [
    "The shapes of `X_bootstrap` and `y_bootstrap` should of course be the same as the shapes of `X` and `y`, but containing randomly selected samples. If you need to convince yourself, you can also print some elements of `X` to see if they are different between runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a2a40-da63-47ff-b950-739a2314dd00",
   "metadata": {},
   "source": [
    "### 1.2 Implement Feature Sampling (2 Points)\n",
    "\n",
    "Implement the method `perform_feature_sampling()`! The input is feature array `X`; use the common approach introduced in the lecture for calculating the number of sampled features -- that is, the number of sample features $m = \\lceil\\sqrt{d}\\rceil$. Apart from the new dataset `X_sample` the method also returns the *indices* of the selected features; we need those for the next task. Hint: Again, numpy should be your best friend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5742cc84-b8c4-4d83-a75f-20e798a52b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_sampled: (150, 2)\n",
      "Selected indices: [1 3]\n"
     ]
    }
   ],
   "source": [
    "def perform_feature_sampling(X):\n",
    "    N, d = X.shape\n",
    "    \n",
    "    X_feature_sampled, selected_indices = None, None\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "    # Calculate the number of sampled features, m\n",
    "    m = int(np.ceil(np.sqrt(d)))\n",
    "    # Shuffle feature indices to select a random subset of features\n",
    "    feature_indices = np.arange(d)\n",
    "    np.random.shuffle(feature_indices)\n",
    "    \n",
    "    # Select the first m indices as the sampled features\n",
    "    selected_indices = feature_indices[:m]\n",
    "    \n",
    "    # Use the selected feature indices to extract the sampled features from X\n",
    "    X_feature_sampled = X[:, selected_indices]\n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################    \n",
    "    \n",
    "    return X_feature_sampled, selected_indices\n",
    "    \n",
    "X_sampled, selected_indices = perform_feature_sampling(X)\n",
    "\n",
    "print('Shape of X_sampled: {}'.format(X_sampled.shape))\n",
    "print('Selected indices: {}'.format(selected_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2311868-b59e-4de0-8114-e3563b0146b9",
   "metadata": {},
   "source": [
    "`X_sampled` has to contain the same number of data samples as `X`, but with less features than `X`. The number of selected indices should of course be reflected in the shape of `X`. For example, if the shape of `X` is $(n, m)$, then there should be $m$ selected indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f61fc-378a-4792-86ac-32765abe5857",
   "metadata": {},
   "source": [
    "### 1.3 Comparing Bagging and Bagging+FeatureSampling (6 Points)\n",
    "\n",
    "Intuitively, different sampled dataset will yield different Decision Trees, not only regarding the accuracy, but also how the Decision Trees will \"look like\". In the following, we train a set of Decision Trees (using the Decision Tree implementation from `sklearn`) based on different dataset samples.\n",
    "\n",
    "In the code cell below, we use our implementations of the auxiliary methods `create_boostrap_sample()` and `perform_feature_sampling()` to train a series of Decision Trees (i.e., a Tree Ensemble) using only Bagging as well as using Bagging + Feature Sampling. In the output, *root index* is the index of the feature used for the very first split, and *#nodes* reflects the total number of nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df219876-bcc2-4ccb-ba3f-6c06603b4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging only\t\t\t\tBaggin + Feature Sampling\n",
      "root index: 3,  #nodes: 15\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 15\t\troot index: 0,  #nodes: 57\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 29\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 25\n",
      "root index: 2,  #nodes: 15\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 27\n",
      "root index: 2,  #nodes: 9\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 21\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 21\n",
      "root index: 3,  #nodes: 7\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 15\t\troot index: 3,  #nodes: 19\n",
      "root index: 3,  #nodes: 15\t\troot index: 0,  #nodes: 61\n",
      "root index: 2,  #nodes: 9\t\troot index: 2,  #nodes: 15\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 19\n",
      "root index: 2,  #nodes: 13\t\troot index: 3,  #nodes: 15\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 17\n",
      "root index: 2,  #nodes: 19\t\troot index: 3,  #nodes: 17\n",
      "root index: 2,  #nodes: 11\t\troot index: 3,  #nodes: 17\n",
      "root index: 3,  #nodes: 13\t\troot index: 2,  #nodes: 29\n",
      "root index: 3,  #nodes: 15\t\troot index: 3,  #nodes: 13\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"Bagging only\\t\\t\\t\\tBaggin + Feature Sampling\")\n",
    "\n",
    "for _ in range(20):\n",
    "    # Create a new bootstrap sample (we can use the same for both ensembles)\n",
    "    X_t, y_t = create_bootstrap_sample(X, y)\n",
    "    classifier_bagging = DecisionTreeClassifier().fit(X_t, y_t)\n",
    "        \n",
    "    # Perform feature sampling on bootstrap sample\n",
    "    X_t, selected_indices = perform_feature_sampling(X_t)\n",
    "    classifier_sampling = DecisionTreeClassifier().fit(X_t, y_t)\n",
    "    \n",
    "    # Print core features of trained Decision Tree\n",
    "    # (feature index of root node, total of number in Decision Trr)\n",
    "    print('root index: {},  #nodes: {}\\t\\troot index: {},  #nodes: {}'\n",
    "          .format(classifier_bagging.tree_.feature[0], classifier_bagging.tree_.node_count,\n",
    "                  selected_indices[classifier_sampling.tree_.feature[0]], classifier_sampling.tree_.node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645db041-fd10-4816-b656-edc571968081",
   "metadata": {},
   "source": [
    "**Interpret the result!** When comparing the resulting Decision Trees when using only **Bagging** and **Bagging+FeatureSampling** you must have observed several differences. List all your observations together with a brief explanation for the observed difference. What insights into the dataset can you gain from your observations?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064ac70-4598-4f13-a3b2-5166aca400d8",
   "metadata": {},
   "source": [
    "\n",
    "* Total Number of Nodes\n",
    "\n",
    "    Bagging tends to produce decision trees with a lower number of nodes, whereas Bagging + Feature Sampling results in trees with more nodes. This is because the random feature sampling introduced more feature diversity in each tree, leading to more complex trees. Bagging, on the other hand, leads to simpler trees because some predictors are very strong, resulting in similar trees.\n",
    "\n",
    "* Tree Complexity\n",
    "\n",
    "    Bagging + Feature Sampling often results in more complex trees with a larger number of nodes. The complexity allows the model to capture more intricate patterns in the data. In contrast, Bagging can produce simpler trees that might underfit the data.\n",
    "\n",
    "* Insights\n",
    "\n",
    "    The dataset probably contains features of varying importance, complex relationships, and diverse patterns. Probably there are some features that are strong predictors.ata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc671f4a-59ae-41bd-bbd4-6cea9c075535",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5413b8a-3fef-4cb6-99ff-f88e6dcee636",
   "metadata": {},
   "source": [
    "## 2 Implementing a AdaBoost with Decision Stumps (30 Points)\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is an ensemble learning method that combines the predictions of multiple base learners to improve overall classification performance. In AdaBoost, the base learners are typically weak learners, which are models that perform slightly better than random guessing (e.g., Decision Stumps). Here's a brief overview of how AdaBoost works using Decision Trees as base learners for classification tasks:\n",
    "\n",
    "* **Initialization:** Assign equal weights to all training samples. These weights determine the importance of each sample in the training process.\n",
    "\n",
    "* **Iterative Phase:**\n",
    "\n",
    "    * *Base Learner Training:* Train a base learner (usually a Decision Stump) on the training data. It tries to minimize the weighted classification error, giving more weight to misclassified samples.\n",
    "\n",
    "    * *Weighted Error Calculation:* Calculate the weighted classification error of the base learner. This error is the sum of the weights of misclassified samples.\n",
    "\n",
    "    * *Classifier Weight Calculation:* Assign a weight to the base learner based on its performance. The better the performance, the higher the weight. This weight is used to determine the contribution of the base learner's prediction in the final ensemble.\n",
    "\n",
    "    * *Update Sample Weights:* Increase the weights of misclassified samples so that they become more important in the next iteration. This focuses the subsequent base learners on the samples that are harder to classify correctly.\n",
    "\n",
    "    * *Ensemble Building:* Combine the predictions of all base learners, weighted by their individual classifier weights, to obtain the final ensemble prediction.\n",
    "\n",
    "* **Final Prediction:** The final prediction is made by aggregating the weighted predictions of all base learners.\n",
    "\n",
    "AdaBoost's strength lies in its ability to focus on the difficult-to-classify examples, allowing it to improve performance even with weak base learners. This makes it particularly effective in situations where a single base learner might struggle. Keep in mind that while AdaBoost is powerful, it's important to be cautious about overfitting. AdaBoost can overfit if the base learners are too complex or if the number of iterations is too high. Therefore, it's advisable to monitor the performance on a validation set and potentially use techniques like early stopping or limiting the complexity of base learners.\n",
    "\n",
    "Your last task will be to implement an AdaBoost Classifier using Decision Stumps as covered in the lecture. But not to worry, this may only sound more difficult than it actually is, and we will guide you through this process step by step. We also keep things simple by assuming that all input features are numerical values.\n",
    "\n",
    "Fundamentally, we can split the implementation into 2 subtasks.\n",
    "\n",
    "* **Weak Learner:** You first implement the simplest \"Decision Stump Classifier\", i.e., a Decision Tree with only one split and therefore a height of 1. This means we do not have to care about the recursive splitting of nodes; it's not complicated but would only add tedious coding.\n",
    "* **AdaBoost:** With the Decision Stump Classifier in place, you can implement AdaBoost as shown in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12989e24-9943-4799-9d0e-0cd36c21448e",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Again, we use the [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset here.\n",
    "\n",
    "#### Load Dataset from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca04abbf-91c6-4eb3-b06d-75e882e35ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.8          2.8           5.1          2.4        2\n",
       "1           6.0          2.2           4.0          1.0        1\n",
       "2           5.5          4.2           1.4          0.2        0\n",
       "3           7.3          2.9           6.3          1.8        2\n",
       "4           5.0          3.4           1.5          0.2        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df_iris = pd.read_csv('data/a3-iris.csv')\n",
    "# Convert the 3 string class labels to 0, 1, and 2\n",
    "df_iris.species = df_iris.species.factorize()[0]\n",
    "df_iris = df_iris.sample(frac=1).reset_index(drop=True)\n",
    "# Show sample of dataset\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea35f1-368b-445f-9615-d4a55bdd633f",
   "metadata": {},
   "source": [
    "#### Convert Dataframe to NumPy arrays + Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda2a2fe-25e2-45bc-997d-270bafb821bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "X = df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()\n",
    "y = df_iris['species'].to_numpy().squeeze()\n",
    "\n",
    "training_size = int(0.8 * X.shape[0])\n",
    "\n",
    "X_train, y_train = X[:training_size], y[:training_size]\n",
    "X_test, y_test = X[training_size:], y[training_size:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8cea0-007b-471d-b04d-9ab4eab864cd",
   "metadata": {},
   "source": [
    "### 2.1 Implementing a Decision Stump Classifier (12 Points)\n",
    "\n",
    "A Decision Stump is nothing else but a Decision Tree with typically only very few splits -- well, more generally, a Decision Tree with a (very) small height, but we keep it simple here. This means that we consider the smallest Decision Stump consisting of only a single split. This means that there is no need to continue recursively splitting child nodes like for a (full) Decision Tree trained to be a Strong Learner.\n",
    "\n",
    "For finding the best split, we need two main things\n",
    "* A scoring method to quantify how good a split is.\n",
    "* A method to actually find the best split (using the scoring method).\n",
    "\n",
    "You can find the skeleton code for the class `DecisionStumpClassifier` implementing the Decision Stump Classifier in the imported `py` file. You will need to complete this code step by step along with the subtask 2.1 a-d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64eae8-efa2-4921-813e-6ac1f9ebce35",
   "metadata": {},
   "source": [
    "#### 2.1 a) Calculating the Gini Score of a Single Node (2 Points)\n",
    "\n",
    "Recall from the lecture, that the Gini score of a node $t$ is defined as:\n",
    "\n",
    "$$Gini(t) = 1 - \\sum_{c\\in C} P(c|t)^2$$\n",
    "\n",
    "where $C$ is the set of classes, and $P(c|t)$ is the relative frequency of class $c$ in node $t$.\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_node()`!** Hint: Have a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and basic `numpy` methods such as [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) and [`np.square`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d1dc5d-f32f-4bda-9b8a-d284b7aaa52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "y1 = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "y2 = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "y3 = np.array([2, 0, 1, 1, 2, 2, 0, 2])\n",
    "\n",
    "print(stump.calc_gini_score_node(y1)) # 0.0\n",
    "print(stump.calc_gini_score_node(y2)) # 0.5\n",
    "print(stump.calc_gini_score_node(y3)) # 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d5dbb-393a-4d50-8283-65e070e01732",
   "metadata": {},
   "source": [
    "#### 2.1 b) Calculating the Gini Score for a Split (2 Points)\n",
    "\n",
    "In the lecture, we defined the impurity of a split as the average of the impurities of the child nodes, weighted by their size (in terms of the number of samples in each child node). Since we only consider binary splits (2 child nodes) and consider only the Gini score to measure impurity, the Gini score of a split simplifies to:\n",
    "\n",
    "$$Gini(t_{left}, t_{right}) = \\frac{n_{left}}{n}Gini(t_{left}) + \\frac{n_{right}}{n}Gini(t_{right})$$\n",
    "\n",
    "where $n_{left}$ ($n_{right}$) is the number of samples in the left (right) child node; and $n = n_{left} + n_{right}$\n",
    "\n",
    "**Implement this formula in the method `calc_gini_score_split`!** You obviously can and should use the existing method `calc_gini_score_node()`. You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec2c5d09-e78e-458e-ab5f-2ad413eb38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "0.3125\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier()\n",
    "\n",
    "print(stump.calc_gini_score_split(y1, y1))  # 0.0\n",
    "print(stump.calc_gini_score_split(y1, y2))  # 0.25\n",
    "print(stump.calc_gini_score_split(y1, y3))  # 0.3125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c975fd-2552-4071-b96f-73b0a69c44d0",
   "metadata": {},
   "source": [
    "#### 2.1 c) Training the Decision Stump: Finding the Best Split (5 Points)\n",
    "\n",
    "With the means to calculate the Gini score for an arbitrary split, we can now train our Decision Stump Classifier. Recall, that our Decision Tree will only have a height of 1 as we only need to make 1 split.\n",
    "\n",
    "**Implement method `fit()`** to find the best split with respect to all features and corresponding thresholds. You obviously can and should use of the existing method `calc_gini_score_split()`. The skeleton code of method `fit()` already provides with the nested loop that goes through all features and the respective thresholds. Note that we keep it simple here as we use all unique values of a features as candidate thresholds.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method. The comments indicate the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385322ba-a8ac-4a48-84c9-4194b38a15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of best feature: 2\n",
      "Best threshold: 2.45\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"Index of best feature:\", stump.feature_idx)\n",
    "print(\"Best threshold:\", stump.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5cd9b-d17e-4fe5-be7c-56626a9da316",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "Index of best feature: 2\n",
    "Best threshold: 2.45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d231267-03e5-48da-bbf7-ce8f77e4796b",
   "metadata": {},
   "source": [
    "#### 2.1 d) Predicting the Class Labels (3 Points)\n",
    "\n",
    "After training our Decision Stump Classifier, we now only need to implement the last step -- that is, the prediction of the class labels for new data samples. Again, since we only have 1 split, this step is also rather easy to implement.\n",
    "\n",
    "**Implement method `predict()`** to predict the class labels for a given set of data samples. Hint: Have again a look at [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) and and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.square.html) to make your life easier. You can use the example calls below to test your implementation of the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44f9595-a004-447c-9324-4cc93a07069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 0. 0. 2. 2. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(stump.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c39a12-e5bf-4305-8979-b6ce9e3ca3ed",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
    " 0. 0. 2. 2. 2. 0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8748b62f-e16f-4698-a558-f82f0a2bad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your Decision Tree implementation on the toy dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "y_pred = stump.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the toy dataset is {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52a989-c417-4e66-95b0-eb46f2737911",
   "metadata": {},
   "source": [
    "The resulting f1 score should be **0.540**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e6c48-4509-4fee-bf3e-c6248323a0de",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** This part is only for you to test your implementation on a real-world dataset (IRIS) since the toy dataset might not reveal all bugs in your code. You can also directly compare the result your Decision Stump implementation with the results from scikit-learn's [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html); Of course, we need to set `max_depth=1` to make it a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb8a27b-29d9-4a8e-94c7-e8c7fbcb45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your Decision Tree implementation on the IRIS dataset is 0.540\n",
      "The f1 score of sklearn Decision Tree implementation on the IRIS dataset is 0.540\n"
     ]
    }
   ],
   "source": [
    "my_stump = DecisionStumpClassifier().fit(X_train, y_train)\n",
    "sk_stump = DecisionTreeClassifier(max_depth=1).fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_stump.predict(X_test)\n",
    "sk_y_pred = sk_stump.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your Decision Tree implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of sklearn Decision Tree implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61b97-eac9-47b2-81d9-3f8082572c0d",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.540** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0839-d1df-40a5-8e15-fa5dfe538d5f",
   "metadata": {},
   "source": [
    "## 2.2 Implementing AdaBoost (12 Points)\n",
    "\n",
    "AdaBoost is a very popular ensemble technique that trains a series of *Weak Learners* to make predictions. Although AdaBoost is a generic technique, it is very commonly used with Decision Stumps as Weak Learners, since Decision Trees with a limited maximum height make naturally good Weak Learners.\n",
    "\n",
    "Again, we provide you with a skeleton code for the class implementing the AdaBoost Classifier. We call it `AdaBoostTreeClassifier` to avoid naming conflicts with `AdaBoostClassifier` of scikit-learn, and because we limit ourselves to Decision Trees (well, Stumps) as the estimators (i.e., the Weak Learners)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9409b98-9379-4b5a-bd39-c1fc38a4dae0",
   "metadata": {},
   "source": [
    "#### 2.1 a) Training the AdaBoost Classifier (8 Points)\n",
    "\n",
    "In the lecture, we went step by step through the training process of an AdaBoost classifier. We saw that this process comprises multiple but rather straightforward steps.\n",
    "\n",
    "\n",
    "**Implement method `fit()`** to train your `AdaBoostTreeClassifier`. The skeleton code of method `fit()` allows you to focus on the core steps within each iteration for training the next Weak Learner (here, our Decision Stump). To help you a little bit, we list the main 4 steps and give you the first step -- training the next estimator using the current dataset sample -- for free.\n",
    "\n",
    "**Important:** By default, `AdaBoostTreeClassifier` uses your implementation of `DecisionStumpClassifier` as its Weak Learner. In case you had problems implementing `DecisionStumpClassifier` or you simply want to test the results, you can also use scikit-learn's `DecisionTreeClassifier`. To make this change, just use the commented line under Step 1 to train the Weak Learner.\n",
    "\n",
    "You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32a73cd4-2e7a-40d2-be68-3b76bef889ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha values -- i.e., the amount-of-says -- are:\n",
      "[0.36544375 0.22696882 0.27751468 0.05696891 0.13179544]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "print(\"The alpha values -- i.e., the amount-of-says -- are:\")\n",
    "print(adaboost.alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56104f15-a1bb-4775-b178-559df6fad71c",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "The alpha values -- i.e., the amount-of-says -- are:\n",
    "[0.36544375 0.54110924 0.65028309 0.65364937 0.53673646]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31473312-a14c-4ad4-b7a5-40914965af8d",
   "metadata": {},
   "source": [
    "#### 2.2 b) Predicting the Class Labels (5 Points)\n",
    "\n",
    "As the last step, we now only need our AdaBoost classifier to predict the class labels for unseen data samples. Again, we saw in the lecture how this works: For each data sample, we check which of the `n_estimators` estimators predicts a certain class label, and the sum of all the alphas (i.e., the amounts of say) of the estimators of the same class.\n",
    "\n",
    "The skeleton code of the `AdaBoostTreeClassifier` already provides a method `predict()` which takes a list of data samples as input and calls the method `predict_sample()` to predict the class label for each data sample individually. It's not that difficult to do this completely vectorized without the loop over the data samples, but here we want to focus on the basic algorithm and not worry about performance.\n",
    "\n",
    "**Implement method predict_sample()** to predict the class label for a given data sample. Hint: Have again a look at [np.argwhere](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html) to maybe make your life easier. You can use the example calls below to test your implementation of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f037f2b0-2dbb-444a-9064-e9bf287e443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 0 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 0 0 0 2 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random\n",
    "np.random.seed(0)\n",
    "\n",
    "adaboost = AdaBoostTreeClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "print(adaboost.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f3b08-9358-475b-819a-fd54330b45bb",
   "metadata": {},
   "source": [
    "The output of previous cell should be:\n",
    "\n",
    "```\n",
    "[0 2 0 0 2 0 2 1 1 1 2 2 1 2 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdb6f2-f184-419c-a271-f307da7f6fbd",
   "metadata": {},
   "source": [
    "**Testing your Implementation on the IRIS Dataset.** We the code cell below, you can again directly compare the result your AdaBoost implementation with the scikit-learn [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). Two things to note:\n",
    "\n",
    "* By default, `AdaBoostClassifier` uses the `DecisionTreeClassifier` class as the estimators (i.e., the Weak Learners) with `max_depth=1`\n",
    "\n",
    "* By default, `AdaBoostClassifier` sets `n_estimators=50`; we therefore choose the same default value for `AdaBoostTreeClassifier`\n",
    "\n",
    "As a result, we do not have to set any parameters for `AdaBoostClassifier` and can use the default ones to allow for a fair comparison with your implementation of `AdaBoostTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b52fc85-d2cf-4a3f-9b8b-11a25034bc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of your AdaBoost implementation on the IRIS dataset is 0.540\n",
      "The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is 0.933\n"
     ]
    }
   ],
   "source": [
    "my_adaboost = AdaBoostTreeClassifier().fit(X_train, y_train)\n",
    "sk_adaboost = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "my_y_pred = my_adaboost.predict(X_test)\n",
    "sk_y_pred = sk_adaboost.predict(X_test)\n",
    "\n",
    "my_f1 = f1_score(y_test, my_y_pred, average='macro')\n",
    "sk_f1 = f1_score(y_test, sk_y_pred, average='macro')\n",
    "\n",
    "print('The f1 score of your AdaBoost implementation on the IRIS dataset is {:.3f}'.format(my_f1))\n",
    "print('The f1 score of the sklearn AdaBoost implementation on the IRIS dataset is {:.3f}'.format(sk_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14028b-e090-47cf-870a-2b11e3645b8e",
   "metadata": {},
   "source": [
    "You should see an f1 score of **0.933** using both your implementation as well as the one from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649193e-eb67-4fac-88d0-b7450aff1a17",
   "metadata": {},
   "source": [
    "### 2.3 Questions about AdaBoost (6 Points)\n",
    "\n",
    "Assume you use your implementation of `AdaBoostTreeClassifier` to train a binary classifier of the dataset shown in 1.1 b).\n",
    "\n",
    "**2.3 a) Question (2 Points):** Will the binary classifier be able to achieve a training error (not test error!) of 0? Explain your answer! (Your explanation is more important than a simple Yes/No answer)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a596b-add5-4acf-aa83-3755f6c4583c",
   "metadata": {},
   "source": [
    "No, the binary classifier trained using AdaBoostTreeClassifier is unlikely to achieve a training error of 0. AdaBoost focuses on repeatedly improving the classification of previously misclassified samples in each iteration. If there are any training examples that are difficult to classify correctly even by a weak learner, they will continue to be weighted more heavily in subsequent rounds. This means that even after multiple rounds of boosting, there is no guarantee that all training examples will be classified correctly.\r\n",
    "\r\n",
    "In other words, AdaBoost aims to reduce the training error, but it doesn't necessarily drive it to zero because it may still have difficulty with certain samples that are inherently noisy, outliers, or simply challenging for the chosen weak learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d899b-c099-4f05-b769-2d3d5c66fd59",
   "metadata": {},
   "source": [
    "**2.3 b) True/False Questions (4 Points):** In the table below are 4 statement that are either *True* or *False*. Complete the table to specify whether a statement is *True* or *False*, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b080cf-91f9-477c-95cf-3827fbf96294",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(4).\n",
    "\n",
    "| No. | Statement                                                                                               \t| True or False?   \t| Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | AdaBoost usually performs better than Random Forests when the dataset contains mislabeled data points | False | AdaBoost is sensitive to noisy data and outliers, so it's likely to perform worse than Random Forests when the dataset contains mislabeled data points. |\n",
    "| (2)  | The error rate $\\epsilon_m$ of the Adaboost classifier always decreases from one iteration to the next. | False | The error rate of the AdaBoost classifier does not always decrease with each iteration. It may initially increase before decreasing as the algorithm focuses on correcting the misclassified samples. |\n",
    "| (3)  | Assume an error rate of $\\epsilon_m \\leq 0.2$ in iteration $m$. This means that up to 20% of the data samples have been misclassified | True | An error rate of error <= 0.2 in iteration m indicates that up to 20% of the data samples have been misclassified. It represents the proportion of misclassified samples. |\n",
    "| (4)  | If after running AdaBoost the last Weak Learner does not misclassify and training samples, additional iterations could still help reduce errors on unseen data  | True | Additional iterations in AdaBoost can still help reduce errors on unseen data, even if the last Weak Learner does not misclassify any training samples. The algorithm may focus on refining the decision boundary and improving the classification of difficult examples. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b77b56-8212-47a9-85e4-dae447301bbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ac1ea-25f6-4d20-84d3-b3cec2f15d23",
   "metadata": {},
   "source": [
    "## 3 Evaluating Tree-Based Models\n",
    "\n",
    "In this last part, we look into evaluation different tree-based models using k-fold cross validationa. K-fold cross-validation is a technique used in machine learning to assess the performance and generalization ability of a model. It involves dividing the dataset into K subsets (or \"folds\") of equal size. The model is trained on K-1 of these folds and tested on the remaining one. This process is repeated K times, with each fold used as the test set exactly once. The final performance metric is computed by averaging the results from each iteration. K-fold cross-validation helps ensure that the model's performance is consistent across different subsets of the data, reducing the risk of overfitting or underfitting.\n",
    "\n",
    "### Prepare Dataset\n",
    "\n",
    "#### Load Dataset from File\n",
    "\n",
    "We use a [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset for this task. Note that we cleaned the dataset for you (i.e., there are no dirty records in there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "643c9a75-171d-40e5-b060-eb6da2a5e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>under-five deaths</th>\n",
       "      <th>...</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "      <th>Life expectancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>Developing</td>\n",
       "      <td>263.0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>71.279624</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>19.1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>8.16</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>584.259210</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.479</td>\n",
       "      <td>10.1</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>271.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.523582</td>\n",
       "      <td>62.0</td>\n",
       "      <td>492</td>\n",
       "      <td>18.6</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>8.18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>612.696514</td>\n",
       "      <td>327582.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.476</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>Developing</td>\n",
       "      <td>268.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.219243</td>\n",
       "      <td>64.0</td>\n",
       "      <td>430</td>\n",
       "      <td>18.1</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>8.13</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>631.744976</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.470</td>\n",
       "      <td>9.9</td>\n",
       "      <td>59.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>78.184215</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2787</td>\n",
       "      <td>17.6</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>8.52</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>669.959000</td>\n",
       "      <td>3696958.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>9.8</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>Developing</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.097109</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3013</td>\n",
       "      <td>17.2</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>7.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.537231</td>\n",
       "      <td>2978599.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>9.5</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n",
       "0  2015  Developing            263.0             62     0.01   \n",
       "1  2014  Developing            271.0             64     0.01   \n",
       "2  2013  Developing            268.0             66     0.01   \n",
       "3  2012  Developing            272.0             69     0.01   \n",
       "4  2011  Developing            275.0             71     0.01   \n",
       "\n",
       "   percentage expenditure  Hepatitis B  Measles   BMI  under-five deaths  ...  \\\n",
       "0               71.279624         65.0     1154  19.1                 83  ...   \n",
       "1               73.523582         62.0      492  18.6                 86  ...   \n",
       "2               73.219243         64.0      430  18.1                 89  ...   \n",
       "3               78.184215         67.0     2787  17.6                 93  ...   \n",
       "4                7.097109         68.0     3013  17.2                 97  ...   \n",
       "\n",
       "   Total expenditure  Diphtheria  HIV/AIDS         GDP  Population  \\\n",
       "0               8.16        65.0       0.1  584.259210  33736494.0   \n",
       "1               8.18        62.0       0.1  612.696514    327582.0   \n",
       "2               8.13        64.0       0.1  631.744976  31731688.0   \n",
       "3               8.52        67.0       0.1  669.959000   3696958.0   \n",
       "4               7.87        68.0       0.1   63.537231   2978599.0   \n",
       "\n",
       "   thinness  1-19 years  thinness 5-9 years  Income composition of resources  \\\n",
       "0                  17.2                17.3                            0.479   \n",
       "1                  17.5                17.5                            0.476   \n",
       "2                  17.7                17.7                            0.470   \n",
       "3                  17.9                18.0                            0.463   \n",
       "4                  18.2                18.2                            0.454   \n",
       "\n",
       "   Schooling  Life expectancy  \n",
       "0       10.1             65.0  \n",
       "1       10.0             59.9  \n",
       "2        9.9             59.9  \n",
       "3        9.8             59.5  \n",
       "4        9.5             59.2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a3-life-expectancy-cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acef995-4e5c-43d4-91d7-2d3ce24a73b0",
   "metadata": {},
   "source": [
    "#### Separate Features & Target\n",
    "\n",
    "For your convenience, we split the dataframe into two, one containing the input features, the other containing the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6b13268-9ac1-4990-b96e-89b3c4589487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1649 samples with 20 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    65.0\n",
       "1    59.9\n",
       "2    59.9\n",
       "3    59.5\n",
       "4    59.2\n",
       "Name: Life expectancy, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = df.iloc[:,0:-1]\n",
    "df_y = df.iloc[:,-1]\n",
    "\n",
    "num_samples, num_features = df_X.shape\n",
    "\n",
    "print('The dataset contains {} samples with {} features'.format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b40b6b-0113-4838-8f31-06a30d2ee54a",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing (2 Points)\n",
    "\n",
    "As usual, the first step is data preprocessing (informed by an EDA). As mentioned above, there's not much to do as this dataset does not contain any \"dirty\" records, particularly, there are no NA values in any of the columns/features. As such, there should be no need to remove any samples.\n",
    "\n",
    "**Perform and data preprocessing/transformation steps you deem appropriate!** As it might affect your decision, the data will be used to train different tree-based models (recall: the tree-based classifiers of sklearn do not support categorical features!). Note that some preprocessing steps might be easier to perform on the pandas dataframe while others on the NumPy arrays. This is why we provide 2 code cells, but it's up to which one to use.\n",
    "\n",
    "**Note:** Perform only preprocessing steps that are indeed needed, and briefly(!) explain your decision by commenting your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da0cabed-075b-4289-a503-102b6b384a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>under-five deaths</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>263.0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>71.279624</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>19.1</td>\n",
       "      <td>83</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.16</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>584.259210</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.479</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>271.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.523582</td>\n",
       "      <td>62.0</td>\n",
       "      <td>492</td>\n",
       "      <td>18.6</td>\n",
       "      <td>86</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>612.696514</td>\n",
       "      <td>327582.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.476</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>268.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.219243</td>\n",
       "      <td>64.0</td>\n",
       "      <td>430</td>\n",
       "      <td>18.1</td>\n",
       "      <td>89</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.13</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>631.744976</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.470</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>78.184215</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2787</td>\n",
       "      <td>17.6</td>\n",
       "      <td>93</td>\n",
       "      <td>67.0</td>\n",
       "      <td>8.52</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>669.959000</td>\n",
       "      <td>3696958.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.097109</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3013</td>\n",
       "      <td>17.2</td>\n",
       "      <td>97</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.537231</td>\n",
       "      <td>2978599.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Status  Adult Mortality  infant deaths  Alcohol  \\\n",
       "0  2015       1            263.0             62     0.01   \n",
       "1  2014       1            271.0             64     0.01   \n",
       "2  2013       1            268.0             66     0.01   \n",
       "3  2012       1            272.0             69     0.01   \n",
       "4  2011       1            275.0             71     0.01   \n",
       "\n",
       "   percentage expenditure  Hepatitis B  Measles   BMI  under-five deaths  \\\n",
       "0               71.279624         65.0     1154  19.1                 83   \n",
       "1               73.523582         62.0      492  18.6                 86   \n",
       "2               73.219243         64.0      430  18.1                 89   \n",
       "3               78.184215         67.0     2787  17.6                 93   \n",
       "4                7.097109         68.0     3013  17.2                 97   \n",
       "\n",
       "   Polio  Total expenditure  Diphtheria  HIV/AIDS         GDP  Population  \\\n",
       "0    6.0               8.16        65.0       0.1  584.259210  33736494.0   \n",
       "1   58.0               8.18        62.0       0.1  612.696514    327582.0   \n",
       "2   62.0               8.13        64.0       0.1  631.744976  31731688.0   \n",
       "3   67.0               8.52        67.0       0.1  669.959000   3696958.0   \n",
       "4   68.0               7.87        68.0       0.1   63.537231   2978599.0   \n",
       "\n",
       "   thinness  1-19 years  thinness 5-9 years  Income composition of resources  \\\n",
       "0                  17.2                17.3                            0.479   \n",
       "1                  17.5                17.5                            0.476   \n",
       "2                  17.7                17.7                            0.470   \n",
       "3                  17.9                18.0                            0.463   \n",
       "4                  18.2                18.2                            0.454   \n",
       "\n",
       "   Schooling  \n",
       "0       10.1  \n",
       "1       10.0  \n",
       "2        9.9  \n",
       "3        9.8  \n",
       "4        9.5  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the \"Status\" column\n",
    "df_X['Status'] = label_encoder.fit_transform(df_X['Status'])\n",
    "df_X.head()\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fbae2-44cb-44ee-9868-9978dcd155dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to numpy arrays\n",
    "X, y = df_X.to_numpy(), df_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6ae36-908c-46af-839e-39c0946f1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf90b6-e701-4c0a-8acd-c0067c1a4ecc",
   "metadata": {},
   "source": [
    "### 3.2 Basic K-Fold Cross Validation\n",
    "\n",
    "The code cell below performs K-Fold Cross Validation. Note that we use `X` and `y` here, and assume our true test data for the final evaluation of the model(s) is a separate dataset. Since we only perform validation here, we can ignore the test data.\n",
    "\n",
    "The code cell below allows you to train a `DecisionTreeRegressor`, a `RandomForestRegressor`, or a `GradientBoostingRegressor` (all `sklearn` implementations). You only need to remove the comment before the regressor of choice, and comment the 2 other regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91afc3b3-73ea-4004-bbed-e40b73d1b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param = 1, RSME training = 0.1 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 2, RSME training = 0.1 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 3, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 5, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.2)\n",
      "param = 8, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 10, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 12, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 15, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 20, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.2)\n",
      "param = 25, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "param = 50, RSME training = 0.0 (0.0), RSME validation = 0.2 (0.1)\n",
      "CPU times: total: 5.41 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Only considered hyperparameter: max depth of trees\n",
    "param_choices = [1, 2, 3, 5, 8, 10, 12, 15, 20, 25, 50]\n",
    "\n",
    "# Keep track of results for visualization\n",
    "param_to_scores = {}\n",
    "\n",
    "for param in param_choices:\n",
    "\n",
    "    # Train regressor with the current parameter setting\n",
    "    regressor = DecisionTreeRegressor(max_depth=param)\n",
    "    #regressor = RandomForestRegressor(max_depth=param)\n",
    "    regressor = GradientBoostingRegressor(max_depth=param)\n",
    "    \n",
    "    # Perform 10-fold cross_validations\n",
    "    scores = cross_validate(regressor, X, y, cv=10, scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Extract the 10 RSME scores (training scores and validation scores) for each run/fold\n",
    "    # The (-1) is only needed since we get the negative root mean squared errors (it's a sklearn thing)\n",
    "    rsme_train = scores['train_score'] * (-1)\n",
    "    rsme_valid = scores['test_score'] * (-1)\n",
    "    \n",
    "    ## Keep track of all num_folds f1 scores for current param (for plotting)\n",
    "    param_to_scores[param] = (rsme_train, rsme_valid)\n",
    "    \n",
    "    ## Print statement for some immediate feedback (values in parenthesis represent the Standard Deviation)\n",
    "    print('param = {}, RSME training = {:.1f} ({:.1f}), RSME validation = {:.1f} ({:.1f})'\n",
    "          .format(param, np.mean(rsme_train), np.std(rsme_train), np.mean(rsme_valid), np.std(rsme_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95be1c2-ea9f-4d24-8611-90e220798a37",
   "metadata": {},
   "source": [
    "**Visualization of Results.** We provide you with 2 methods to visualize the results:\n",
    "* `plot_validation_results()` shows all `num_folds` scores for each parameter setting together with the means and standard deviations of the validation scores.\n",
    "* `plot_scores()` shows the training and validation scores for each parameter setting.\n",
    "\n",
    "Just run the code cell below to plot both figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "86510a8f-530d-4027-8507-8f2e11416909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_validation_results(param_to_scores)\n",
    "\n",
    "plot_scores(param_to_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fbca4-ff7a-475a-b104-0fc212e4a5b7",
   "metadata": {},
   "source": [
    "#### 3.2 a) Comparing Tree-Based Regression Models (5 Points)\n",
    "\n",
    "Run the k-fold cross validation for all 3 regressors and compare and discuss the results! You should see quite a number of differences regarding runtimes, issues of overfitting and underfitting, overall performance, effects of parameter values, etc. You can use the code cells above for cross validation and visualization.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b75e07-30d4-47cc-ae35-30e583a85658",
   "metadata": {},
   "source": [
    "##### DecisionTree Regressor:\n",
    "\n",
    "* Runtime: The DecisionTreeRegressor has the fastest runtime.\n",
    "* Overfitting: With a low max depth, it exhibits high training and validation error, indicating underfitting. As the max depth increases, the training error becomes zero, suggesting overfitting.\n",
    "* Performance: The DecisionTreeRegressor may not perform well due to overfitting when the max depth is too high.\n",
    "* Parameter Effect: Increasing the max depth can lead to overfitting.\n",
    "\n",
    "##### RandomForest Regressor:\n",
    "\n",
    "* Runtime: The RandomForestRegressor has a longer runtime compared to the DecisionTreeRegressor.\n",
    "* Overfitting: Similar to the DecisionTreeRegressor, as the max depth increases, the training error decreases to zero, indicating overfitting. However, the validation error is lower than the DecisionTreeRegressor.\n",
    "* Performance: RandomForestRegressor generally performs better than DecisionTreeRegressor due to the ensemble nature that reduces overfitting.\n",
    "* Parameter Effect: Increasing the max depth can still lead to overfitting, but it's less severe than a single Decision Tree.\n",
    "\n",
    "##### GradientBoosting Regressor:\n",
    "\n",
    "* Runtime: The GradientBoostingRegressor has a longer runtime compared to the DecisionTreeRegressor but is faster than the RandomForestRegressor.\n",
    "* Overfitting: As the max depth increases, the training error decreases, but the validation error remains relatively stable. This suggests that GradientBoosting is less prone to overfitting compared to the previous models.\n",
    "* Performance: GradientBoostingRegressor generally performs the best, as it achieves lower validation errors.\n",
    "* Parameter Effect: Increasing the max depth has less impact on overfitting compared to the previous models.ting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe03c5b-dde9-4a18-8b6f-7de32d977d69",
   "metadata": {},
   "source": [
    "#### 3.2 b) Assessing the Evaluation (3 Points)\n",
    "\n",
    "Discuss if we found the regressor with the cross-validation result from above! There is no need to implement anything here.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec8767-9a8d-4450-a2e1-cb006eb1c57b",
   "metadata": {},
   "source": [
    "Based on the cross-validation results, we found that the GradientBoostingRegressor is the most promising model for the given task. It provides good performance, is less susceptible to overfitting, and offers a reasonable runtime. However, it's essential to fine-tune the hyperparameters of the GradientBoostingRegressor to further improve its performance, and it's possible that other regressors or more advanced ensemble techniques could provide even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0f5c9-d95d-4cbd-a642-794ddbf64d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
